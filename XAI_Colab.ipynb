{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI Pipeline - Google Colab Notebook\n",
    "## Explainable AI for AWS CloudTrail Analysis\n",
    "\n",
    "This notebook runs the XAI pipeline without Ollama/local LLMs.\n",
    "Focus: **Quantitative validation of XAI techniques (SHAP, LIME)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q shap lime sentence-transformers transformers joblib scikit-learn scipy tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (if using Drive for models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository\n",
    "!git clone https://github.com/anuradha-gh/xai-saas-access-control.git\n",
    "%cd xai-saas-access-control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Paths (Update These!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: If models are in Google Drive\n",
    "MODEL_PATH = '/content/drive/MyDrive/SAAS_XAI/'  # Update this path\n",
    "\n",
    "CONFIG = {\n",
    "    'c1_autoencoder': MODEL_PATH + 'autoencoder.h5',\n",
    "    'c1_iso_forest': MODEL_PATH + 'isolation_forest.joblib',\n",
    "    'c2_bert_path': MODEL_PATH + 'trained_role_classifier/checkpoint-15000',\n",
    "    'c3_sbert_path': MODEL_PATH + 'c3_unsupervised_aws_model/sbert_model',\n",
    "    'c3_iso_forest': MODEL_PATH + 'c3_unsupervised_aws_model/isolation_forest.joblib',\n",
    "    'log_data': MODEL_PATH + 'flaws_cloudtrail00.json'\n",
    "}\n",
    "\n",
    "# Option B: Upload models directly to Colab (for small files)\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure XAI (No LLM Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable LLM for Colab\n",
    "USE_LLM = False  # No Ollama needed\n",
    "LOCAL_MODEL_NAME = None\n",
    "\n",
    "XAI_CONFIG = {\n",
    "    'enable_xai': True,\n",
    "    'default_stakeholder': 'technical',  # Doesn't matter without LLM\n",
    "    'enable_validation': True,  # IMPORTANT: Enable validation\n",
    "    'num_shap_samples': 100,\n",
    "    'num_lime_samples': 1000,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ XAI configured for Colab (LLM disabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import XAI Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import XAI modules\n",
    "from xai_explainer import XAIExplainerFactory, SHAPExplainer, LIMETextExplainer\n",
    "from llm_translator import LLMTranslator, StakeholderType\n",
    "from xai_validator import XAIValidator\n",
    "\n",
    "print(\"‚úÖ XAI modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "state = {'models': {}, 'preprocessors': {}}\n",
    "\n",
    "print(\"üìÇ Loading CloudTrail data for preprocessing...\")\n",
    "with open(CONFIG['log_data'], 'r') as f:\n",
    "    log_data = json.load(f)\n",
    "\n",
    "# Helper functions (from XAI.py)\n",
    "def parse_log_c1(record):\n",
    "    return {\n",
    "        'eventName': record.get('eventName', 'Unknown'),\n",
    "        'eventSource': record.get('eventSource', 'Unknown').split('.')[0],\n",
    "        'userIdentityType': record.get('userIdentity', {}).get('type', 'Unknown'),\n",
    "        'awsRegion': record.get('awsRegion', 'Unknown'),\n",
    "    }\n",
    "\n",
    "# Prepare preprocessing\n",
    "records = log_data.get('Records', [])[:1000]  # Use subset for Colab\n",
    "df = pd.DataFrame([parse_log_c1(r) for r in records])\n",
    "cat_features = ['eventName', 'eventSource', 'userIdentityType', 'awsRegion']\n",
    "for col in cat_features:\n",
    "    df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "# C1 Preprocessing\n",
    "print(\"üîß Fitting C1 preprocessors...\")\n",
    "preprocessor = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)], remainder='passthrough')\n",
    "X_processed = preprocessor.fit_transform(df).toarray()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_processed)\n",
    "state['preprocessors']['c1_prep'] = preprocessor\n",
    "state['preprocessors']['c1_scaler'] = scaler\n",
    "\n",
    "# Load C1 Models\n",
    "print(\"üß† Loading C1 Autoencoder...\")\n",
    "state['models']['c1_ae'] = load_model(CONFIG['c1_autoencoder'])\n",
    "print(\"üå≤ Loading C1 IsolationForest...\")\n",
    "state['models']['c1_if'] = joblib.load(CONFIG['c1_iso_forest'])\n",
    "dense_layers = [l for l in state['models']['c1_ae'].layers if isinstance(l, tf.keras.layers.Dense)]\n",
    "bottleneck = min(dense_layers, key=lambda l: l.units)\n",
    "state['models']['c1_enc'] = tf.keras.models.Model(inputs=state['models']['c1_ae'].input, outputs=bottleneck.output)\n",
    "\n",
    "# Load C2 Models\n",
    "print(\"ü§ñ Loading C2 BERT model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['c2_bert_path'])\n",
    "c2_model = AutoModelForSequenceClassification.from_pretrained(CONFIG['c2_bert_path'])\n",
    "state['models']['c2_pipe'] = pipeline(\"text-classification\", model=c2_model, tokenizer=tokenizer, return_all_scores=True)\n",
    "\n",
    "# Load C3 Models\n",
    "print(\"üìù Loading C3 Sentence-BERT...\")\n",
    "state['models']['c3_sbert'] = SentenceTransformer(CONFIG['c3_sbert_path'])\n",
    "print(\"üå≤ Loading C3 IsolationForest...\")\n",
    "state['models']['c3_if'] = joblib.load(CONFIG['c3_iso_forest'])\n",
    "\n",
    "print(\"\\n‚úÖ All models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize XAI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Initializing XAI Pipeline...\")\n",
    "\n",
    "# Prepare background data for SHAP (latent + MSE features)\n",
    "X_scaled = scaler.transform(X_processed[:100])\n",
    "latent = state['models']['c1_enc'].predict(X_scaled, verbose=0)\n",
    "recon = state['models']['c1_ae'].predict(X_scaled, verbose=0)\n",
    "mse = np.mean((X_scaled - recon)**2, axis=1).reshape(-1, 1)\n",
    "c1_features_for_shap = np.hstack([latent, mse])\n",
    "\n",
    "# Create explainer factory\n",
    "state['xai_explainer'] = XAIExplainerFactory(state['models'], state['preprocessors'])\n",
    "background_data = {'c1_features': c1_features_for_shap}\n",
    "state['xai_explainer'].initialize(background_data)\n",
    "\n",
    "# Create LLM translator (template mode)\n",
    "state['llm_translator'] = LLMTranslator(use_ollama=False)\n",
    "\n",
    "# Create validator\n",
    "state['xai_validator'] = XAIValidator()\n",
    "\n",
    "print(\"‚úÖ XAI Pipeline Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test XAI on Example Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CloudTrail log\n",
    "test_log = {\n",
    "    \"eventTime\": \"2017-02-12T21:30:56Z\",\n",
    "    \"eventSource\": \"s3.amazonaws.com\",\n",
    "    \"eventName\": \"DeleteBucket\",\n",
    "    \"awsRegion\": \"us-west-2\",\n",
    "    \"sourceIPAddress\": \"AWS Internal\",\n",
    "    \"userIdentity\": {\n",
    "        \"type\": \"Root\",\n",
    "        \"userName\": \"root_account\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare features\n",
    "p_log = parse_log_c1(test_log)\n",
    "processed = state['preprocessors']['c1_prep'].transform(pd.DataFrame([p_log])).toarray()\n",
    "scaled = state['preprocessors']['c1_scaler'].transform(processed)\n",
    "\n",
    "# Extract latent + MSE\n",
    "latent = state['models']['c1_enc'].predict(scaled, verbose=0)\n",
    "recon = state['models']['c1_ae'].predict(scaled, verbose=0)\n",
    "mse = np.mean((scaled - recon)**2, axis=1).reshape(-1, 1)\n",
    "features_for_if = np.hstack([latent, mse])\n",
    "\n",
    "# Get XAI explanation\n",
    "xai_result = state['xai_explainer'].explain_c1(features_for_if, p_log)\n",
    "\n",
    "print(\"\\nüìä XAI EXPLANATION:\")\n",
    "print(\"=\"*70)\n",
    "if 'shap' in xai_result:\n",
    "    print(\"\\nüîç SHAP Feature Importance:\")\n",
    "    for feat in xai_result['shap']['feature_importance'][:5]:\n",
    "        print(f\"  - {feat['feature']}: {feat['shap_value']:.4f} (importance: {feat['importance']:.4f})\")\n",
    "\n",
    "if 'reconstruction' in xai_result:\n",
    "    print(\"\\nüîß Reconstruction Errors:\")\n",
    "    for feat in xai_result['reconstruction']['feature_errors'][:3]:\n",
    "        print(f\"  - {feat['feature']}: {feat['error']:.4f} (value: {feat['original_value']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run XAI Validation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî¨ RUNNING XAI VALIDATION SUITE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare test data\n",
    "test_records = log_data.get('Records', [])[:50]\n",
    "df_test = pd.DataFrame([parse_log_c1(r) for r in test_records])\n",
    "for col in cat_features:\n",
    "    df_test[col] = df_test[col].fillna('Unknown')\n",
    "\n",
    "X_test_processed = state['preprocessors']['c1_prep'].transform(df_test).toarray()\n",
    "X_test_scaled = state['preprocessors']['c1_scaler'].transform(X_test_processed)\n",
    "\n",
    "# Extract latent + MSE for test data\n",
    "latent_test = state['models']['c1_enc'].predict(X_test_scaled, verbose=0)\n",
    "recon_test = state['models']['c1_ae'].predict(X_test_scaled, verbose=0)\n",
    "mse_test = np.mean((X_test_scaled - recon_test)**2, axis=1).reshape(-1, 1)\n",
    "c1_test_features = np.hstack([latent_test, mse_test])\n",
    "\n",
    "test_data = {'c1': c1_test_features}\n",
    "\n",
    "# Run validation\n",
    "report = state['xai_validator'].validate_all(state['models'], state['xai_explainer'], test_data)\n",
    "\n",
    "# Display results\n",
    "print(state['xai_validator'].generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get SHAP values for multiple instances\n",
    "explainer = state['xai_explainer'].get_explainer('c1_shap')\n",
    "shap_results = []\n",
    "\n",
    "for i in range(min(10, len(c1_test_features))):\n",
    "    result = explainer.explain(c1_test_features[i:i+1])\n",
    "    shap_results.append(result)\n",
    "\n",
    "# Extract feature names and values\n",
    "feature_names = [f['feature'] for f in shap_results[0]['feature_importance']]\n",
    "shap_values = np.array([[f['shap_value'] for f in r['feature_importance']] for r in shap_results])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, np.mean(np.abs(shap_values), axis=0))\n",
    "plt.xlabel('Mean Absolute SHAP Value')\n",
    "plt.title('Feature Importance for C1 Anomaly Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Validation Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "with open('validation_report_colab.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "# Download to local machine\n",
    "from google.colab import files\n",
    "files.download('validation_report_colab.json')\n",
    "\n",
    "print(\"‚úÖ Validation report saved and downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "- ‚úÖ Running XAI pipeline in Colab **without Ollama**\n",
    "- ‚úÖ **SHAP explanations** with feature importance\n",
    "- ‚úÖ **Validation metrics** (fidelity & stability)\n",
    "- ‚úÖ **Visualization** of results\n",
    "- ‚úÖ Works with free Colab GPU\n",
    "\n",
    "**Key Insight**: You don't need LLM for XAI validation! The numerical metrics (SHAP values, perturbation sensitivity, Jaccard similarity) are the **ground truth** for XAI quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
