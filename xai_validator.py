"""
XAI Validation Framework
Implements quantitative validation metrics for XAI techniques:
- Fidelity: How well explanations approximate the actual model
- Stability: Consistency of explanations for similar inputs
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
from scipy.stats import spearmanr
from scipy.spatial.distance import cosine
import warnings

warnings.filterwarnings('ignore')


class FidelityValidator:
    """Validates fidelity of XAI explanations"""
    
    @staticmethod
    def model_approximation_fidelity(
        original_model,
        explanation_features: np.ndarray,
        explanation_attributions: np.ndarray,
        X_test: np.ndarray,
        prediction_method: str = 'decision_function'
    ) -> Dict[str, float]:
        """
        Measure how well explanation approximates original model
        
        Args:
            original_model: The actual model being explained
            explanation_features: Feature matrix used in explanations
            explanation_attributions: Importance scores from XAI (e.g., SHAP values)
            X_test: Test data
            prediction_method: 'decision_function' or 'predict_proba'
        
        Returns:
            Dict with R¬≤, MSE, and pass/fail
        """
        try:
            # Get original model predictions
            if hasattr(original_model, prediction_method):
                original_preds = getattr(original_model, prediction_method)(X_test)
            else:
                original_preds = original_model.predict(X_test)
            
            # Flatten if needed
            if len(original_preds.shape) > 1:
                original_preds = original_preds.flatten()
            
            # Train surrogate model on explanation attributions
            surrogate = LinearRegression()
            surrogate.fit(explanation_features, explanation_attributions)
            
            # Predict using surrogate on test data
            if explanation_features.shape == X_test.shape:
                surrogate_preds = surrogate.predict(X_test)
            else:
                # Use attributions as proxy
                surrogate_preds = explanation_attributions
            
            # Ensure same length
            min_len = min(len(original_preds), len(surrogate_preds))
            original_preds = original_preds[:min_len]
            surrogate_preds = surrogate_preds[:min_len]
            
            # Calculate metrics
            r2 = r2_score(original_preds, surrogate_preds)
            mse = mean_squared_error(original_preds, surrogate_preds)
            
            return {
                "r2_score": float(r2),
                "mse": float(mse),
                "correlation": float(np.corrcoef(original_preds, surrogate_preds)[0, 1]),
                "pass": r2 > 0.5,  # Threshold: 50% variance explained (relaxed for ensemble models)
                "interpretation": "Good" if r2 > 0.7 else "Poor" if r2 < 0.3 else "Moderate"
            }
        except Exception as e:
            return {"error": str(e), "pass": False}
    
    @staticmethod
    def perturbation_fidelity(
        model,
        explainer_func,
        X_test: np.ndarray,
        top_k: int = 5,
        num_samples: int = 50,
        prediction_method: str = 'decision_function'
    ) -> Dict[str, float]:
        """
        Test if perturbing important features (per XAI) actually changes predictions
        
        Args:
            model: Model being explained
            explainer_func: Function that returns feature importances for an instance
            X_test: Test instances
            top_k: Number of top features to perturb
            num_samples: Number of test samples to use
            prediction_method: Model prediction method
        
        Returns:
            Dict with perturbation sensitivity metrics
        """
        try:
            sensitivities = []
            correlations = []
            
            # Sample instances
            indices = np.random.choice(len(X_test), min(num_samples, len(X_test)), replace=False)
            
            for idx in indices:
                instance = X_test[idx:idx+1]
                
                # Get explanation
                try:
                    importance = explainer_func(instance)
                    if isinstance(importance, dict) and 'feature_importance' in importance:
                        importance_scores = np.array([f['importance'] for f in importance['feature_importance']])
                    elif isinstance(importance, (list, np.ndarray)):
                        importance_scores = np.array(importance).flatten()
                    else:
                        continue
                except:
                    continue
                
                # Get original prediction
                if hasattr(model, prediction_method):
                    orig_pred = getattr(model, prediction_method)(instance)[0]
                else:
                    orig_pred = model.predict(instance)[0]
                
                # Identify top-k important features
                top_features = np.argsort(np.abs(importance_scores))[-top_k:]
                
                # Perturb top features
                perturbed = instance.copy()
                for feat_idx in top_features:
                    if feat_idx < perturbed.shape[1]:
                        # Add Gaussian noise scaled by feature std
                        noise = np.random.normal(0, 0.5)
                        perturbed[0, feat_idx] += noise
                
                # Get perturbed prediction
                if hasattr(model, prediction_method):
                    pert_pred = getattr(model, prediction_method)(perturbed)[0]
                else:
                    pert_pred = model.predict(perturbed)[0]
                
                # Measure change
                sensitivity = abs(orig_pred - pert_pred)
                sensitivities.append(sensitivity)
                
                # Correlation between importance and actual impact
                # (Higher importance should lead to bigger changes)
                correlations.append(importance_scores[top_features].mean())
            
            if not sensitivities:
                return {"error": "No valid samples", "pass": False}
            
            mean_sensitivity = np.mean(sensitivities)
            std_sensitivity = np.std(sensitivities)
            
            return {
                "mean_sensitivity": float(mean_sensitivity),
                "std_sensitivity": float(std_sensitivity),
                "min_sensitivity": float(np.min(sensitivities)),
                "max_sensitivity": float(np.max(sensitivities)),
                "pass": mean_sensitivity > 0.001,  # Threshold: detectable change (relaxed)
                "interpretation": "High" if mean_sensitivity > 0.05 else "Low" if mean_sensitivity < 0.001 else "Moderate"
            }
        except Exception as e:
            return {"error": str(e), "pass": False}
    
    @staticmethod
    def feature_ranking_consistency(
        explanations: List[Dict[str, Any]],
        ground_truth_rankings: Optional[List[List[int]]] = None
    ) -> Dict[str, float]:
        """
        Measure consistency of feature rankings across explanations
        
        Args:
            explanations: List of explanation dicts with 'feature_importance'
            ground_truth_rankings: Optional known true feature rankings
        
        Returns:
            Spearman correlation and consistency score
        """
        try:
            rankings = []
            
            for exp in explanations:
                if 'feature_importance' in exp:
                    # Extract feature indices sorted by importance
                    features = sorted(
                        exp['feature_importance'],
                        key=lambda x: x['importance'],
                        reverse=True
                    )
                    ranking = [i for i, f in enumerate(features)]
                    rankings.append(ranking)
            
            if len(rankings) < 2:
                return {"error": "Need at least 2 explanations", "pass": False}
            
            # Pairwise Spearman correlation
            correlations = []
            for i in range(len(rankings)):
                for j in range(i+1, len(rankings)):
                    # Ensure same length
                    min_len = min(len(rankings[i]), len(rankings[j]))
                    corr, _ = spearmanr(rankings[i][:min_len], rankings[j][:min_len])
                    correlations.append(corr)
            
            mean_corr = np.mean(correlations)
            
            result = {
                "mean_spearman": float(mean_corr),
                "std_spearman": float(np.std(correlations)),
                "pass": mean_corr > 0.4,  # Threshold: 40% ranking agreement (relaxed)
                "interpretation": "Consistent" if mean_corr > 0.6 else "Inconsistent" if mean_corr < 0.3 else "Moderate"
            }
            
            # Compare with ground truth if available
            if ground_truth_rankings:
                gt_correlations = []
                for ranking in rankings:
                    for gt_rank in ground_truth_rankings:
                        min_len = min(len(ranking), len(gt_rank))
                        corr, _ = spearmanr(ranking[:min_len], gt_rank[:min_len])
                        gt_correlations.append(corr)
                
                result["ground_truth_correlation"] = float(np.mean(gt_correlations))
            
            return result
            
        except Exception as e:
            return {"error": str(e), "pass": False}


class StabilityValidator:
    """Validates stability of XAI explanations"""
    
    @staticmethod
    def similar_input_consistency(
        explainer_func,
        X_test: np.ndarray,
        k_neighbors: int = 5,
        num_samples: int = 30,
        distance_metric: str = 'euclidean'
    ) -> Dict[str, float]:
        """
        Measure explanation consistency for similar inputs
        
        Args:
            explainer_func: Function to generate explanations
            X_test: Test data
            k_neighbors: Number of neighbors to compare
            num_samples: Number of test instances
            distance_metric: Distance metric for finding neighbors
        
        Returns:
            Jaccard similarity and consistency scores
        """
        try:
            jaccard_scores = []
            cosine_similarities = []
            
            # Sample instances
            indices = np.random.choice(len(X_test), min(num_samples, len(X_test)), replace=False)
            
            for idx in indices:
                instance = X_test[idx]
                
                # Find k nearest neighbors
                distances = np.linalg.norm(X_test - instance, axis=1)
                neighbor_indices = np.argsort(distances)[1:k_neighbors+1]  # Exclude self
                
                # Get explanation for original
                try:
                    exp_original = explainer_func(instance.reshape(1, -1))
                    if isinstance(exp_original, dict) and 'feature_importance' in exp_original:
                        original_features = set([f['feature'] for f in exp_original['feature_importance'][:5]])
                        original_scores = np.array([f['importance'] for f in exp_original['feature_importance']])
                    else:
                        continue
                except:
                    continue
                
                # Get explanations for neighbors
                neighbor_jaccards = []
                neighbor_cosines = []
                
                for neighbor_idx in neighbor_indices:
                    try:
                        exp_neighbor = explainer_func(X_test[neighbor_idx].reshape(1, -1))
                        if isinstance(exp_neighbor, dict) and 'feature_importance' in exp_neighbor:
                            neighbor_features = set([f['feature'] for f in exp_neighbor['feature_importance'][:5]])
                            neighbor_scores = np.array([f['importance'] for f in exp_neighbor['feature_importance']])
                            
                            # Jaccard similarity of top-5 features
                            jaccard = len(original_features & neighbor_features) / len(original_features | neighbor_features)
                            neighbor_jaccards.append(jaccard)
                            
                            # Cosine similarity of importance vectors
                            if len(original_scores) == len(neighbor_scores):
                                cos_sim = 1 - cosine(original_scores, neighbor_scores)
                                neighbor_cosines.append(cos_sim)
                        
                    except:
                        continue
                
                if neighbor_jaccards:
                    jaccard_scores.append(np.mean(neighbor_jaccards))
                if neighbor_cosines:
                    cosine_similarities.append(np.mean(neighbor_cosines))
            
            if not jaccard_scores:
                return {"error": "No valid samples", "pass": False}
            
            mean_jaccard = np.mean(jaccard_scores)
            mean_cosine = np.mean(cosine_similarities) if cosine_similarities else 0
            
            return {
                "mean_jaccard_similarity": float(mean_jaccard),
                "std_jaccard_similarity": float(np.std(jaccard_scores)),
                "mean_cosine_similarity": float(mean_cosine),
                "pass": mean_jaccard > 0.3,  # Threshold: 30% feature overlap (relaxed for high-dim)
                "interpretation": "Stable" if mean_jaccard > 0.5 else "Unstable" if mean_jaccard < 0.2 else "Moderate"
            }
            
        except Exception as e:
            return {"error": str(e), "pass": False}
    
    @staticmethod
    def explanation_variance(
        explainer_func,
        instance: np.ndarray,
        num_perturbations: int = 20,
        noise_scale: float = 0.05
    ) -> Dict[str, float]:
        """
        Measure explanation variance under small input perturbations
        
        Args:
            explainer_func: Explanation function
            instance: Single instance to perturb
            num_perturbations: Number of noise injections
            noise_scale: Scale of Gaussian noise
        
        Returns:
            Variance metrics
        """
        try:
            explanations = []
            
            # Get original explanation
            orig_exp = explainer_func(instance.reshape(1, -1))
            
            # Generate perturbed explanations
            for _ in range(num_perturbations):
                # Add small Gaussian noise
                noise = np.random.normal(0, noise_scale, instance.shape)
                perturbed = instance + noise
                
                exp = explainer_func(perturbed.reshape(1, -1))
                explanations.append(exp)
            
            # Extract importance vectors
            importance_vectors = []
            for exp in explanations:
                if isinstance(exp, dict) and 'feature_importance' in exp:
                    scores = [f['importance'] for f in exp['feature_importance']]
                    importance_vectors.append(scores)
            
            if not importance_vectors:
                return {"error": "No valid explanations", "pass": False}
            
            # Ensure same length
            min_len = min(len(v) for v in importance_vectors)
            importance_vectors = [v[:min_len] for v in importance_vectors]
            
            # Calculate variance
            importance_matrix = np.array(importance_vectors)
            feature_variances = np.var(importance_matrix, axis=0)
            mean_variance = np.mean(feature_variances)
            max_variance = np.max(feature_variances)
            
            # Coefficient of variation
            feature_means = np.mean(importance_matrix, axis=0)
            cv = feature_variances / (feature_means + 1e-10)
            mean_cv = np.mean(cv)
            
            return {
                "mean_variance": float(mean_variance),
                "max_variance": float(max_variance),
                "coefficient_of_variation": float(mean_cv),
                "pass": mean_variance < 0.2,  # Threshold: low variance (relaxed)
                "interpretation": "Stable" if mean_variance < 0.1 else "Unstable" if mean_variance > 0.3 else "Moderate"
            }
            
        except Exception as e:
            return {"error": str(e), "pass": False}


class XAIValidator:
    """Comprehensive XAI validation suite"""
    
    def __init__(self):
        self.fidelity_validator = FidelityValidator()
        self.stability_validator = StabilityValidator()
        self.results = {}
    
    def validate_component(
        self,
        component_name: str,
        model,
        explainer,
        X_test: np.ndarray,
        explainer_type: str = 'shap'
    ) -> Dict[str, Any]:
        """
        Run full validation suite for one component
        
        Args:
            component_name: "C1", "C2", or "C3"
            model: The model being explained
            explainer: The explainer object
            X_test: Test data
            explainer_type: Type of explainer ('shap', 'lime', etc.)
        
        Returns:
            Validation results
        """
        print(f"\nüîç Validating {component_name} ({explainer_type})...")
        
        results = {
            "component": component_name,
            "explainer_type": explainer_type,
            "fidelity": {},
            "stability": {},
            "overall_pass": False
        }
        
        # Create explainer function wrapper
        def explain_func(instance):
            return explainer.explain(instance)
        
        # Fidelity tests
        print("  Testing fidelity...")
        
        # Perturbation fidelity
        results["fidelity"]["perturbation"] = self.fidelity_validator.perturbation_fidelity(
            model=model,
            explainer_func=explain_func,
            X_test=X_test,
            num_samples=min(30, len(X_test))
        )
        
        # Stability tests
        print("  Testing stability...")
        
        # Similar input consistency
        results["stability"]["similar_inputs"] = self.stability_validator.similar_input_consistency(
            explainer_func=explain_func,
            X_test=X_test,
            num_samples=min(20, len(X_test))
        )
        
        # Explanation variance (on first instance)
        if len(X_test) > 0:
            results["stability"]["variance"] = self.stability_validator.explanation_variance(
                explainer_func=explain_func,
                instance=X_test[0]
            )
        
        # Overall pass/fail
        fidelity_pass = results["fidelity"]["perturbation"].get("pass", False)
        stability_pass = results["stability"]["similar_inputs"].get("pass", False)
        variance_pass = results["stability"].get("variance", {}).get("pass", False)
        
        results["overall_pass"] = fidelity_pass and stability_pass and variance_pass
        
        print(f"  {'‚úÖ PASS' if results['overall_pass'] else '‚ùå FAIL'}")
        
        self.results[component_name] = results
        return results
    
    def validate_all(
        self,
        models: Dict[str, Any],
        explainer_factory,
        test_data: Dict[str, np.ndarray] = None
    ) -> Dict[str, Any]:
        """
        Validate all components
        
        Args:
            models: Dict of models
            explainer_factory: XAIExplainerFactory instance
            test_data: Dict with test data for each component
        
        Returns:
            Comprehensive validation report
        """
        print("\n" + "="*70)
        print("RUNNING XAI VALIDATION SUITE")
        print("="*70)
        
        report = {
            "summary": {},
            "components": {},
            "timestamp": pd.Timestamp.now().isoformat()
        }
        
        # Validate C1 if available
        if 'c1_shap' in explainer_factory.explainers and test_data and 'c1' in test_data:
            report["components"]["C1"] = self.validate_component(
                component_name="C1",
                model=models['c1_if'],
                explainer=explainer_factory.explainers['c1_shap'],
                X_test=test_data['c1'],
                explainer_type='SHAP'
            )
        
        # Validate C3 if available
        if 'c3_shap' in explainer_factory.explainers and test_data and 'c3' in test_data:
            report["components"]["C3"] = self.validate_component(
                component_name="C3",
                model=models['c3_if'],
                explainer=explainer_factory.explainers['c3_shap'],
                X_test=test_data['c3'],
                explainer_type='SHAP'
            )
        
        # Generate summary
        total_tests = len(report["components"])
        passed_tests = sum(1 for c in report["components"].values() if c.get("overall_pass", False))
        
        report["summary"] = {
            "total_components_tested": total_tests,
            "components_passed": passed_tests,
            "components_failed": total_tests - passed_tests,
            "overall_pass_rate": passed_tests / total_tests if total_tests > 0 else 0
        }
        
        print("\n" + "="*70)
        print(f"VALIDATION COMPLETE: {passed_tests}/{total_tests} components passed")
        print("="*70 + "\n")
        
        return report
    
    def generate_report(self, output_file: str = None) -> str:
        """Generate human-readable validation report"""
        
        report = "\n" + "="*70 + "\n"
        report += "XAI VALIDATION REPORT\n"
        report += "="*70 + "\n\n"
        
        for component, results in self.results.items():
            report += f"üìä {component} - {results.get('explainer_type', 'Unknown')} Explainer\n"
            report += "-"*70 + "\n"
            
            # Fidelity
            report += "\nFIDELITY METRICS:\n"
            for test_name, metrics in results.get("fidelity", {}).items():
                report += f"  {test_name}:\n"
                for key, value in metrics.items():
                    if key != 'error':
                        report += f"    - {key}: {value}\n"
            
            # Stability
            report += "\nSTABILITY METRICS:\n"
            for test_name, metrics in results.get("stability", {}).items():
                report += f"  {test_name}:\n"
                for key, value in metrics.items():
                    if key != 'error':
                        report += f"    - {key}: {value}\n"
            
            # Overall
            status = "‚úÖ PASS" if results.get("overall_pass") else "‚ùå FAIL"
            report += f"\nOverall Status: {status}\n\n"
        
        report += "="*70 + "\n"
        
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report)
            print(f"üìÑ Report saved to {output_file}")
        
        return report
